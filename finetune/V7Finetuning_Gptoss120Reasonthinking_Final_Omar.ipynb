{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SefQOerQgYEx"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/EnhancedReasoning_gptoss120B')\n",
        "\n",
        "print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loading\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "MODEL_NAME = \"unsloth/gpt-oss-20b\"\n",
        "MAX_SEQ_LEN = 4096\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LEN}\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    dtype=None,  # Auto detection\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "iC-VnxOLlA-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                   \"7.mlp.experts.gate_up_proj\", \"7.mlp.experts.down_proj\",\n",
        "                   \"15.mlp.experts.gate_up_proj\", \"15.mlp.experts.down_proj\",\n",
        "                   \"23.mlp.experts.gate_up_proj\", \"23.mlp.experts.down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Set up tokenizer\n",
        "print(\"Configuring tokenizer...\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "aEDvR3bilDuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Process the enhanced CSV with GPToss120B reasoning\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the enhanced CSV with GPToss120B reasoning\n",
        "print(\"Loading enhanced CSV with GPToss120B reasoning...\")\n",
        "enhanced_df = pd.read_csv('Enhanced_Training_Data_with_GPToss120B_Reasoning.csv')\n",
        "\n",
        "print(f\"Loaded {len(enhanced_df)} cases\")\n",
        "print(f\"Cases with reasoning: {enhanced_df['reasoning_matched'].sum()}\")\n",
        "\n",
        "# Filter to only cases with reasoning (matched ones)\n",
        "matched_df = enhanced_df[enhanced_df['reasoning_matched'] == True].copy()\n",
        "print(f\"Processing {len(matched_df)} cases with GPToss120B reasoning\")\n",
        "\n",
        "def enhanced_formatting_prompts_func(examples):\n",
        "    \"\"\"Format with GPToss120B reasoning as thinking content\"\"\"\n",
        "    convos = []\n",
        "\n",
        "    for i in range(len(examples[\"PostDescription\"])):\n",
        "        combined_description = examples[\"PostDescription\"][i]\n",
        "        differential_diagnosis = examples[\"DifferentialDiagnosisList\"][i]\n",
        "        diagnosis = examples[\"FinalDiagnosis\"][i]\n",
        "        reasoning = examples[\"gptoss120b_reasoning\"][i]\n",
        "\n",
        "        # Process differential diagnosis list\n",
        "        dd_list = [dd.strip() for dd in str(differential_diagnosis).split(',')]\n",
        "        dd_formatted = \"\\n\".join(dd_list)\n",
        "\n",
        "        # Create user prompt\n",
        "        user_prompt = f\"\"\"Your task is to make diagnosis based on the following information about a patient.\n",
        "\n",
        "{combined_description}\n",
        "\n",
        "Here is a differential diagnosis list:\n",
        "{dd_formatted}\n",
        "\n",
        "Please give the best diagnosis:\"\"\"\n",
        "\n",
        "        # Create conversation with reasoning as thinking\n",
        "        conversation = [\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": diagnosis, \"thinking\": reasoning}\n",
        "        ]\n",
        "\n",
        "        convos.append(conversation)\n",
        "\n",
        "    return {\"messages\": convos}\n",
        "\n",
        "# Convert to Dataset and apply formatting\n",
        "dataset = Dataset.from_pandas(matched_df)\n",
        "\n",
        "dataset = dataset.map(enhanced_formatting_prompts_func, batched=True)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"PRELIMINARY Context Length Analysis (before chat template):\")\n",
        "\n",
        "# Estimate raw text lengths\n",
        "sample_size = min(100, len(dataset))\n",
        "raw_lengths = []\n",
        "\n",
        "for i in range(sample_size):\n",
        "    messages = dataset[i]['messages']  # Get the full conversation\n",
        "\n",
        "    total_content = \"\"\n",
        "\n",
        "    for message in messages:\n",
        "        # Add user content\n",
        "        if message.get('content'):\n",
        "            total_content += str(message['content'])\n",
        "\n",
        "        # Add thinking content if it exists\n",
        "        if message.get('thinking'):\n",
        "            total_content += str(message['thinking'])\n",
        "\n",
        "    raw_lengths.append(len(total_content))\n",
        "\n",
        "print(f\"Raw text length statistics (chars):\")\n",
        "print(f\"- Min: {min(raw_lengths):,} chars\")\n",
        "print(f\"- Max: {max(raw_lengths):,} chars\")\n",
        "print(f\"- Average: {sum(raw_lengths)/len(raw_lengths):,.0f} chars\")\n",
        "print(f\"- 95th percentile: {sorted(raw_lengths)[int(0.95*len(raw_lengths))]:,} chars\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2xx8z_y4mo9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Apply chat template\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the processed dataset\n",
        "print(\"Loading processed dataset...\")\n",
        "dataset = load_from_disk('processed_dataset_with_gptoss120b_thinking')\n",
        "print(f\"Loaded {len(dataset)} examples\")\n",
        "\n",
        "def final_formatting_func(examples):\n",
        "    \"\"\"Apply chat template with medium reasoning\"\"\"\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = []\n",
        "    for convo in convos:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_token=False,\n",
        "            reasoning_effort=\"medium\"\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply final formatting\n",
        "print(\"Applying chat template...\")\n",
        "dataset = dataset.map(final_formatting_func, batched=True)"
      ],
      "metadata": {
        "id": "T4CW6MMsmvxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvxgWxhCsWIt",
        "outputId": "ed7382cc-91f0-41fa-f50c-d6ebdeda5c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample formatted example:\n",
            "==================================================\n",
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-09-25\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Your task is to make diagnosis based on the following information about a patient.\n",
            "\n",
            "Clinical History: This patient presented subacutely with left thigh pain for two months, and had an outpatient plain pelvic radiograph. She later presented acutely to the hospital after a short interval of having \"heard a loud crack\" in the left leg, then fallen, with no preceding trauma. At this point repeat radiography was performed.\n",
            " \n",
            " Imaging Findings: The outpatient radiograph was initially assessed as normal. Although indeed not demonstrating any femoral or pelvic fracture, it did in fact show focal cortical thickening of the left lateral proximal femur. At this site, one week later, a complete transverse fracture of the proximal femur was seen with some displacement, with features consistent with atypical femoral fracture.\n",
            "\n",
            "Here is a differential diagnosis list:\n",
            "Insufficiency fracture\n",
            "Pathological fracture\n",
            "Bisphosphonate-associated atypical femoral fracture with preceding cortical reaction\n",
            "\n",
            "Please give the best diagnosis:<|end|><|start|>assistant<|channel|>analysis<|message|>Step 1 – Relate the patient’s symptoms to the imaging.  \n",
            "The woman has had a two‑month history of vague left‑thigh discomfort that suddenly progressed to an audible “crack” and a fall. In the outpatient pelvis‑film there is no obvious break, but a focal area of increased density along the lateral cortex of the proximal femur is present. One week later the same region shows a complete transverse fracture through the subtro‑trochanteric segment with minimal displacement. The clinical picture of prodromal pain followed by a low‑energy, “spontaneous” crack is classic for an evolving stress‑type fracture rather than an acute traumatic break.\n",
            "\n",
            "Step 2 – Map findings to each differential.  \n",
            "\n",
            "| Differential | Typical imaging features | How the case fits / conflicts |\n",
            "|--------------|------------------------|------------------------------|\n",
            "| Insufficiency fracture (osteoporotic) | Often occurs in the femoral neck or intertrochanteric region; fracture line may be oblique or spiral; cortical thickening is usually medial or diffuse; no characteristic “beaking” of the lateral cortex. | The lesion is located on the lateral cortex, is focal, and appears as a well‑circumscribed thickening that precedes a transverse fracture. These are not typical for a pure insufficiency fracture. |\n",
            "| Pathological fracture (metastasis, primary bone tumor) | Underlying lytic or sclerotic lesion, peri‑lesional bone destruction, soft‑tissue mass, irregular cortical margins, often eccentric and not confined to the lateral cortex. | The pre‑fracture film shows only cortical thickening without any lucent nidus, periosteal reaction, or soft‑tissue component. The subsequent fracture is clean, transverse, and symmetric, arguing against a destructive process. |\n",
            "| Bisphosphonate‑associated atypical femoral fracture (AFF) with cortical reaction | Long‑term bisphosphonate exposure; prodromal thigh/groin pain; focal lateral cortical thickening (“stress reaction”); fracture line is transverse or short‑oblique, located at the subtro‑trochanteric/metadiaphyseal junction; minimal comminution; often bilateral. | The patient has >10 years of alendronate use, a classic duration of risk. The initial film demonstrates the hallmark lateral cortical beaking; the later film shows a complete transverse subtro‑trochanteric fracture with limited displacement. All imaging and clinical criteria for an AFF are satisfied. |\n",
            "\n",
            "Step 3 – Systematic elimination.  \n",
            "The absence of a lytic or sclerotic lesion, peri‑lesional soft tissue, and the highly localized lateral cortical reaction make a pathological fracture unlikely. The fracture orientation, location, and preceding cortical thickening are not characteristic of a plain insufficiency fracture, which typically lacks a discrete lateral stress reaction. Consequently, these two entities can be confidently excluded.\n",
            "\n",
            "Step 4 – Converge on the final diagnosis.  \n",
            "The combination of long‑term bisphosphonate therapy, subacute thigh pain, focal lateral cortical thickening on the initial radiograph, and the evolution to a transverse subtro‑trochanteric fracture fulfills the major and minor criteria for a bisphosphonate‑associated atypical femoral fracture with preceding cortical reaction. Early recognition of the cortical stress line would have allowed prophylactic fixation and possibly prevented complete fracture.<|end|><|start|>assistant<|channel|>final<|message|>Bisphosphonate-associated atypical femoral fracture with preceding cortical reaction<|return|>\n"
          ]
        }
      ],
      "source": [
        "# Show a formatted example\n",
        "print(\"Sample formatted example:\")\n",
        "print(\"=\"*50)\n",
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thinking Style Training - GPToss120B Reasoning\n",
        "\n",
        "import wandb\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# W&B login\n",
        "wandb.login(key=\"\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=1e-4,\n",
        "        max_seq_length=4096,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        logging_steps=25,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=\"eurorad_thinking_gptoss120b_reasoning\",\n",
        "        output_dir=\"eurorad_thinking_gptoss120b_reasoning\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=10,\n",
        "        gradient_checkpointing=True,\n",
        "        dataloader_num_workers=0,\n",
        "        remove_unused_columns=False,\n",
        "        seed=3407,\n",
        "        data_seed=3407,\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "AQGbfNrUkThl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Starting thinking style training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save LoRA adapter locally (thinking approach)\n",
        "try:\n",
        "    model.save_pretrained(\"eurorad_medical_final_thinking_gptoss120b\")\n",
        "    tokenizer.save_pretrained(\"eurorad_medical_final_thinking_gptoss120b\")\n",
        "    print(\"LoRA adapter thinking saved locally to ./eurorad_medical_final_thinking_gptoss120b/\")\n",
        "except Exception as e:\n",
        "    print(f\"Local LoRA save failed: {e}\")\n",
        "\n",
        "# Create merged model (thinking approach)\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    merged_model = model.merge_and_unload()\n",
        "    merged_model.save_pretrained(\"eurorad_medical_merged_thinking_gptoss120b\")\n",
        "    tokenizer.save_pretrained(\"eurorad_medical_merged_thinking_gptoss120b\")\n",
        "    print(\"Merged model thinking saved locally to ./eurorad_medical_merged_thinking_gptoss120b/\")\n",
        "except Exception as e:\n",
        "    print(f\"Merged model save failed: {e}\")\n",
        "\n",
        "print(\"Training and saving completed!\")"
      ],
      "metadata": {
        "id": "4rfjXdoYkOQ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unsloth_env (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}