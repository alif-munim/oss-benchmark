{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Model Loading\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Use your proven Unsloth setup\n",
        "MODEL_NAME = \"unsloth/gpt-oss-20b\"\n",
        "MAX_SEQ_LEN = 4096\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(f\"Max sequence length: {MAX_SEQ_LEN}\")\n",
        "\n",
        "# Load model with your Unsloth settings\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    dtype=None,  # Auto detection\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        ")"
      ],
      "metadata": {
        "id": "yZ_-yL1vr3Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oct 07\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "                   # All standard linear layers (equivalent to \"all-linear\")\n",
        "                   \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                   \"embed_tokens\", \"lm_head\",\n",
        "\n",
        "                   # Early layers (0-7) - Initial processing\n",
        "                   \"1.mlp.experts.gate_up_proj\", \"1.mlp.experts.down_proj\",\n",
        "                   \"3.mlp.experts.gate_up_proj\", \"3.mlp.experts.down_proj\",\n",
        "                   \"5.mlp.experts.gate_up_proj\", \"5.mlp.experts.down_proj\",\n",
        "                   \"7.mlp.experts.gate_up_proj\", \"7.mlp.experts.down_proj\",\n",
        "\n",
        "                   # Middle layers (8-15) - Pattern recognition & reasoning\n",
        "                   \"9.mlp.experts.gate_up_proj\", \"9.mlp.experts.down_proj\",\n",
        "                   \"11.mlp.experts.gate_up_proj\", \"11.mlp.experts.down_proj\",\n",
        "                   \"13.mlp.experts.gate_up_proj\", \"13.mlp.experts.down_proj\",\n",
        "                   \"15.mlp.experts.gate_up_proj\", \"15.mlp.experts.down_proj\",\n",
        "\n",
        "                   # Upper layers (16-23) - Deep reasoning\n",
        "                   \"17.mlp.experts.gate_up_proj\", \"17.mlp.experts.down_proj\",\n",
        "                   \"19.mlp.experts.gate_up_proj\", \"19.mlp.experts.down_proj\",\n",
        "                   \"21.mlp.experts.gate_up_proj\", \"21.mlp.experts.down_proj\",\n",
        "                   \"23.mlp.experts.gate_up_proj\", \"23.mlp.experts.down_proj\",\n",
        "\n",
        "                   # Deep layers (24-31) - Final refinement & output\n",
        "                   \"25.mlp.experts.gate_up_proj\", \"25.mlp.experts.down_proj\",\n",
        "                   \"27.mlp.experts.gate_up_proj\", \"27.mlp.experts.down_proj\",\n",
        "                   \"29.mlp.experts.gate_up_proj\", \"29.mlp.experts.down_proj\",\n",
        "                   \"31.mlp.experts.gate_up_proj\", \"31.mlp.experts.down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Set up tokenizer\n",
        "print(\"Configuring tokenizer...\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "DwDyISaclQnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9WWn5NAi8f5",
        "outputId": "c8a93054-1229-4cb1-fe1b-1c371d75f14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 22,452,224 || all params: 20,943,736,384 || trainable%: 0.1072\n"
          ]
        }
      ],
      "source": [
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the enhanced CSV with GPToss120B reasoning  Data\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the enhanced CSV with GPToss120B reasoning\n",
        "print(\"Loading enhanced CSV with GPToss120B reasoning...\")\n",
        "enhanced_df = pd.read_csv('Enhanced_Training_Data_with_GPToss120B_Reasoning.csv')\n",
        "\n",
        "print(f\"Loaded {len(enhanced_df)} cases\")\n",
        "print(f\"Cases with reasoning: {enhanced_df['reasoning_matched'].sum()}\")\n",
        "\n",
        "# Filter to only cases with reasoning\n",
        "matched_df = enhanced_df[enhanced_df['reasoning_matched'] == True].copy()\n",
        "print(f\"Processing {len(matched_df)} cases with GPToss120B reasoning\")\n",
        "\n",
        "def enhanced_formatting_prompts_func(examples):\n",
        "    \"\"\"Format with GPToss120B reasoning as thinking content\"\"\"\n",
        "    convos = []\n",
        "\n",
        "    for i in range(len(examples[\"PostDescription\"])):\n",
        "        combined_description = examples[\"PostDescription\"][i]\n",
        "        differential_diagnosis = examples[\"DifferentialDiagnosisList\"][i]\n",
        "        diagnosis = examples[\"FinalDiagnosis\"][i]\n",
        "        reasoning = examples[\"gptoss120b_reasoning\"][i]\n",
        "\n",
        "        # Process differential diagnosis list\n",
        "        dd_list = [dd.strip() for dd in str(differential_diagnosis).split(',')]\n",
        "        dd_formatted = \"\\n\".join(dd_list)\n",
        "\n",
        "        # NEW DETAILED PROMPT\n",
        "        user_prompt = f\"\"\"You are an expert radiologist demonstrating step-by-step diagnostic reasoning.\n",
        "\n",
        "Case presentation:\n",
        "\n",
        "{combined_description}\n",
        "\n",
        "Differential diagnoses to consider:\n",
        "{dd_formatted}\n",
        "\n",
        "Generate systematic Chain-of-Thought reasoning that shows how clinicians think through cases:\n",
        "\n",
        "1. **Connect symptoms to findings**: Link clinical presentation with imaging observations\n",
        "2. **Map to differentials**: Show how findings support or contradict each differential diagnosis\n",
        "3. **Systematic elimination**: Explicitly rule out less likely options with reasoning\n",
        "4. **Converge to answer**: Demonstrate the logical path to the correct diagnosis\"\"\"\n",
        "\n",
        "        # Create conversation with reasoning as thinking\n",
        "        conversation = [\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": diagnosis, \"thinking\": reasoning}\n",
        "        ]\n",
        "\n",
        "        convos.append(conversation)\n",
        "\n",
        "    return {\"messages\": convos}\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Dataset and apply formatting\n",
        "print(\"\\nConverting to HuggingFace Dataset...\")\n",
        "dataset = Dataset.from_pandas(matched_df)\n",
        "\n",
        "print(\"Applying enhanced formatting with GPToss120B reasoning as thinking...\")\n",
        "dataset = dataset.map(enhanced_formatting_prompts_func, batched=True)\n",
        "\n",
        "# Quick context length estimation BEFORE chat template\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"PRELIMINARY Context Length Analysis (before chat template):\")\n",
        "\n",
        "# Estimate raw text lengths\n",
        "sample_size = min(100, len(dataset))\n",
        "raw_lengths = []\n",
        "\n",
        "for i in range(sample_size):\n",
        "    messages = dataset[i]['messages']  # Get the full conversation\n",
        "\n",
        "    total_content = \"\"\n",
        "\n",
        "    for message in messages:\n",
        "        # Add user content\n",
        "        if message.get('content'):\n",
        "            total_content += str(message['content'])\n",
        "\n",
        "        # Add thinking content if it exists\n",
        "        if message.get('thinking'):\n",
        "            total_content += str(message['thinking'])\n",
        "\n",
        "    raw_lengths.append(len(total_content))\n",
        "\n",
        "print(f\"Raw text length statistics (chars):\")\n",
        "print(f\"- Min: {min(raw_lengths):,} chars\")\n",
        "print(f\"- Max: {max(raw_lengths):,} chars\")\n",
        "print(f\"- Average: {sum(raw_lengths)/len(raw_lengths):,.0f} chars\")\n",
        "print(f\"- 95th percentile: {sorted(raw_lengths)[int(0.95*len(raw_lengths))]:,} chars\")\n",
        "\n",
        "# Rough token estimation (1 token ≈ 4 characters for medical text)\n",
        "estimated_tokens = [length // 4 for length in raw_lengths]\n",
        "print(f\"\\nEstimated token lengths:\")\n",
        "print(f\"- Min: {min(estimated_tokens):,} tokens\")\n",
        "print(f\"- Max: {max(estimated_tokens):,} tokens\")\n",
        "print(f\"- Average: {sum(estimated_tokens)/len(estimated_tokens):,.0f} tokens\")\n",
        "print(f\"- 95th percentile: {sorted(estimated_tokens)[int(0.95*len(estimated_tokens))]:,} tokens\")\n",
        "\n",
        "\n",
        "# Save the processed dataset for the next step\n",
        "dataset.save_to_disk('processed_dataset_with_gptoss120b_thinking')\n",
        "print(\"Processed dataset saved to 'processed_dataset_with_gptoss120b_thinking'\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"Ready for final chat template processing!\")\n",
        "print(f\"Dataset info: {dataset_info}\")"
      ],
      "metadata": {
        "id": "uJwnkYEFqH2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply chat template (run after processing enhanced CSV)\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the processed dataset\n",
        "print(\"Loading processed dataset...\")\n",
        "dataset = load_from_disk('processed_dataset_with_gptoss120b_thinking')\n",
        "print(f\"Loaded {len(dataset)} examples\")\n",
        "\n",
        "def final_formatting_func(examples):\n",
        "    \"\"\"Apply chat template with medium reasoning\"\"\"\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = []\n",
        "    for convo in convos:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_token=False,\n",
        "            reasoning_effort=\"medium\"\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply final formatting\n",
        "print(\"Applying chat template...\")\n",
        "dataset = dataset.map(final_formatting_func, batched=True)"
      ],
      "metadata": {
        "id": "nY01AaMSqBem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQmMBPyhi8f9",
        "outputId": "e1d1c5a6-dd77-435d-9802-13d9abf3bf83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample formatted example:\n",
            "==================================================\n",
            "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
            "Knowledge cutoff: 2024-06\n",
            "Current date: 2025-09-25\n",
            "\n",
            "Reasoning: medium\n",
            "\n",
            "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
            "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Your task is to make diagnosis based on the following information about a patient.\n",
            "\n",
            "Clinical History: This patient presented subacutely with left thigh pain for two months, and had an outpatient plain pelvic radiograph. She later presented acutely to the hospital after a short interval of having \"heard a loud crack\" in the left leg, then fallen, with no preceding trauma. At this point repeat radiography was performed.\n",
            " \n",
            " Imaging Findings: The outpatient radiograph was initially assessed as normal. Although indeed not demonstrating any femoral or pelvic fracture, it did in fact show focal cortical thickening of the left lateral proximal femur. At this site, one week later, a complete transverse fracture of the proximal femur was seen with some displacement, with features consistent with atypical femoral fracture.\n",
            "\n",
            "Here is a differential diagnosis list:\n",
            "Insufficiency fracture\n",
            "Pathological fracture\n",
            "Bisphosphonate-associated atypical femoral fracture with preceding cortical reaction\n",
            "\n",
            "Please give the best diagnosis:<|end|><|start|>assistant<|channel|>analysis<|message|>Step 1 – Relate the patient’s symptoms to the imaging.  \n",
            "The woman has had a two‑month history of vague left‑thigh discomfort that suddenly progressed to an audible “crack” and a fall. In the outpatient pelvis‑film there is no obvious break, but a focal area of increased density along the lateral cortex of the proximal femur is present. One week later the same region shows a complete transverse fracture through the subtro‑trochanteric segment with minimal displacement. The clinical picture of prodromal pain followed by a low‑energy, “spontaneous” crack is classic for an evolving stress‑type fracture rather than an acute traumatic break.\n",
            "\n",
            "Step 2 – Map findings to each differential.  \n",
            "\n",
            "| Differential | Typical imaging features | How the case fits / conflicts |\n",
            "|--------------|------------------------|------------------------------|\n",
            "| Insufficiency fracture (osteoporotic) | Often occurs in the femoral neck or intertrochanteric region; fracture line may be oblique or spiral; cortical thickening is usually medial or diffuse; no characteristic “beaking” of the lateral cortex. | The lesion is located on the lateral cortex, is focal, and appears as a well‑circumscribed thickening that precedes a transverse fracture. These are not typical for a pure insufficiency fracture. |\n",
            "| Pathological fracture (metastasis, primary bone tumor) | Underlying lytic or sclerotic lesion, peri‑lesional bone destruction, soft‑tissue mass, irregular cortical margins, often eccentric and not confined to the lateral cortex. | The pre‑fracture film shows only cortical thickening without any lucent nidus, periosteal reaction, or soft‑tissue component. The subsequent fracture is clean, transverse, and symmetric, arguing against a destructive process. |\n",
            "| Bisphosphonate‑associated atypical femoral fracture (AFF) with cortical reaction | Long‑term bisphosphonate exposure; prodromal thigh/groin pain; focal lateral cortical thickening (“stress reaction”); fracture line is transverse or short‑oblique, located at the subtro‑trochanteric/metadiaphyseal junction; minimal comminution; often bilateral. | The patient has >10 years of alendronate use, a classic duration of risk. The initial film demonstrates the hallmark lateral cortical beaking; the later film shows a complete transverse subtro‑trochanteric fracture with limited displacement. All imaging and clinical criteria for an AFF are satisfied. |\n",
            "\n",
            "Step 3 – Systematic elimination.  \n",
            "The absence of a lytic or sclerotic lesion, peri‑lesional soft tissue, and the highly localized lateral cortical reaction make a pathological fracture unlikely. The fracture orientation, location, and preceding cortical thickening are not characteristic of a plain insufficiency fracture, which typically lacks a discrete lateral stress reaction. Consequently, these two entities can be confidently excluded.\n",
            "\n",
            "Step 4 – Converge on the final diagnosis.  \n",
            "The combination of long‑term bisphosphonate therapy, subacute thigh pain, focal lateral cortical thickening on the initial radiograph, and the evolution to a transverse subtro‑trochanteric fracture fulfills the major and minor criteria for a bisphosphonate‑associated atypical femoral fracture with preceding cortical reaction. Early recognition of the cortical stress line would have allowed prophylactic fixation and possibly prevented complete fracture.<|end|><|start|>assistant<|channel|>final<|message|>Bisphosphonate-associated atypical femoral fracture with preceding cortical reaction<|return|>\n"
          ]
        }
      ],
      "source": [
        "# Show a formatted example\n",
        "print(\"Sample formatted example:\")\n",
        "print(\"=\"*50)\n",
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb-QgActi8f-"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# W&B login\n",
        "wandb.login(key=\"\")\n",
        "\n",
        "print(\"Starting Thinking Style Training with GPToss120B Reasoning\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training configuration\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=SFTConfig(\n",
        "        # Core training parameters\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        num_train_epochs=3,\n",
        "\n",
        "        # Learning rate\n",
        "        learning_rate=1e-4,\n",
        "\n",
        "        # Memory and sequence length\n",
        "        max_seq_length=4096,\n",
        "\n",
        "        # Optimization settings\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "\n",
        "        # Logging and monitoring\n",
        "        logging_steps=25,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=\"eurorad_thinking_gptoss120b_reasoning_linearMOEOct7\",\n",
        "\n",
        "        # Saving strategy\n",
        "        output_dir=\"eurorad_thinking_gptoss120b_reasoning_linearMOEOct7\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=10,\n",
        "\n",
        "        # Memory optimization\n",
        "        gradient_checkpointing=True,\n",
        "        dataloader_num_workers=0,\n",
        "        remove_unused_columns=False,\n",
        "\n",
        "        # Reproducibility\n",
        "        seed=3407,\n",
        "        data_seed=3407,\n",
        "\n",
        "        # Precision\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqa2uG9ti8f-"
      },
      "outputs": [],
      "source": [
        "# Oct 7\n",
        "# Start training\n",
        "print(\"Starting thinking style training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHYCh486i8f-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unsloth_env (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}