{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "    load_in_4bit = True, # QLoRA\n",
    ")\n",
    "# Prepare LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32, lora_alpha=64, lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "DATA_DIR = \"/home/alhusain/scratch/ondevice-llm/eurorad\"\n",
    "\n",
    "# Harmony responses template\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": f\"{DATA_DIR}/train_cot.jsonl\",\n",
    "                \"val\":   f\"{DATA_DIR}/val.jsonl\"}\n",
    ")\n",
    "\n",
    "def to_text(batch):\n",
    "    texts = []\n",
    "    for msgs in batch[\"messages\"]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            msgs,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,  # training on full dialogues including the label\n",
    "            reasoning_effort=\"high\"\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "ds = raw.map(to_text, batched=True, remove_columns=raw[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "MAX_SEQ_LEN = 4096\n",
    "OUTPUT_DIR = \"/home/alhusain/scratch/ondevice-llm\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=20,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"val\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    packing=False,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/lora_long_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE = \"openai/gpt-oss-20b\"\n",
    "LORA_PATH = \"/home/alhusain/scratch/ondevice-llm/lora_long_prompt/\"\n",
    "\n",
    "model_base, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE, max_seq_length=4096, load_in_4bit=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model_base, LORA_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"Using PEFT adapter:\", isinstance(model, PeftModel))\n",
    "print(\"Active adapters:\", list(getattr(model, \"peft_config\", {}).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse-beam CoT inference with external per-beam scoring\n",
    "import re, json, math, torch, random, numpy as np, pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "_device = next(model.parameters()).device\n",
    "if _device.type == \"cuda\":\n",
    "    torch.cuda.set_device(_device.index or 0)\n",
    "\n",
    "DATA_DIR = \"/home/alhusain/scratch/ondevice-llm\"\n",
    "VAL_CSV  = f\"{DATA_DIR}/eurorad_val.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(VAL_CSV)\n",
    "#df = df.iloc[101:].copy()\n",
    "\n",
    "# Prompt \n",
    "def make_msgs_from_row(row):\n",
    "    post = str(row.get(\"PostDescription\", \"\")).strip()\n",
    "    ddx  = str(row.get(\"DifferentialDiagnosisList\", \"\")).strip()\n",
    "    user = (\n",
    "        \"You are an expert radiologist. You will receive a case and a finite list of candidate diagnoses. \"\n",
    "        \"Reason step by step, then pick exactly one diagnosis copied verbatim from the list. \"\n",
    "        \"You may use standard radiology knowledge to interpret the given findings (don't invent new findings).\\n\\n\"\n",
    "        f\"PostDescription:\\n{post}\\n\\n\"\n",
    "        f\"DifferentialDiagnosisList:\\n{ddx}\\n\\n\"\n",
    "        \"Response rules:\\n\"\n",
    "        \"1) Provide <analysis> then <final>. No text outside these blocks.\\n\"\n",
    "        \"2) In <final>, copy the chosen label verbatim from the list. Do not include any other text.\\n\\n\"\n",
    "        \"<analysis> content and order:\\n\"\n",
    "        \"A) Key findings (bullets): age/sex; clinical context; organ or site; modality and sequences; morphology and matrix; \"\n",
    "        \"signal/enhancement; pathognomonic phrases; risk factors.\\n\"\n",
    "        \"B) Candidate pass (coverage required): for each i=1..K write\\n\"\n",
    "        \"   - [i] Pros: 1–2 explicit findings that support it (quote phrases from PostDescription).\\n\"\n",
    "        \"   - [i] Cons: 1–2 explicit contradictions or missing must-haves.\\n\"\n",
    "        \"   If the required organ/site is incompatible, mark CONTRADICTION and discard.\\n\"\n",
    "        \"C) Ranking with tie-breaks, in order:\\n\"\n",
    "        \"   1) Pathognomonic or hallmark findings win.\\n\"\n",
    "        \"   2) Exact anatomic site and laterality match.\\n\"\n",
    "        \"   3) Specific subtype over umbrella when supported.\\n\"\n",
    "        \"   4) Age, epidemiology, and stated risk factors.\\n\"\n",
    "        \"   5) If still tied, choose the narrowest diagnosis among remaining.\\n\"\n",
    "        \"D) Sanity check and alignment:\\n\"\n",
    "        \"   - State: Top candidate: [i] LABEL.\\n\"\n",
    "        \"   - Confirm LABEL exists verbatim in the list and matches your ranking.\\n\"\n",
    "        \"</analysis>\\n\\n\"\n",
    "        \"<final> LABEL </final>\\n\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert radiologist.\"},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "    ]\n",
    "\n",
    "# Parsers\n",
    "ANALYSIS_RE = re.compile(r\"<analysis>\\s*(.*?)\\s*</analysis>\", re.I | re.S)\n",
    "FINAL_RE    = re.compile(r\"<final>\\s*(.*?)\\s*</final>\",       re.I | re.S)\n",
    "\n",
    "def parse_analysis_final(text: str):\n",
    "    text = text or \"\"\n",
    "    a = (ANALYSIS_RE.search(text).group(1).strip()\n",
    "         if ANALYSIS_RE.search(text) else \"\")\n",
    "    f = (FINAL_RE.search(text).group(1).strip()\n",
    "         if FINAL_RE.search(text) else \"\")\n",
    "    if not f:\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines: f = lines[-1]\n",
    "    return a, f\n",
    "\n",
    "def parse_final_only(text: str):\n",
    "    m = FINAL_RE.search(text or \"\")\n",
    "    if m: return m.group(1).strip()\n",
    "    lines = [ln.strip() for ln in (text or \"\").splitlines() if ln.strip()]\n",
    "    return lines[-1] if lines else \"\"\n",
    "\n",
    "# Utilities \n",
    "def _model_compute_dtype(model):\n",
    "    for p in model.parameters():\n",
    "        if p is not None:\n",
    "            return p.dtype\n",
    "    return torch.float32\n",
    "\n",
    "# Diverse beams (deterministic)     \n",
    "def gen_cot_beams(msgs, effort=\"high\", max_new_tokens=1024,\n",
    "                  beams=8, diversity_penalty=0.35, length_penalty=1.0, no_repeat_ngram_size=0):\n",
    "    # Encode once; keep these tensors to later score beams under the same context\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=effort,\n",
    "    )\n",
    "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            do_sample=False,                   \n",
    "            num_beams=beams,\n",
    "            num_beam_groups=beams,             \n",
    "            num_return_sequences=beams,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # Return both texts (for parsing) and the raw token IDs (for scoring)\n",
    "    prompt_len = enc[\"input_ids\"].shape[1]\n",
    "    gens, cont_ids_list = [], []\n",
    "    for i in range(out.size(0)):\n",
    "        cont_ids = out[i, prompt_len:] # continuation ids\n",
    "        cont_ids_list.append(cont_ids.unsqueeze(0)) # [1, T_i]\n",
    "        gens.append(tokenizer.decode(cont_ids, skip_special_tokens=True).strip())\n",
    "    return gens, enc, cont_ids_list\n",
    "\n",
    "# External per-beam scoring (length-normalized logprob) \n",
    "@torch.no_grad()\n",
    "def score_continuation_given_ids(model, enc_input_ids, cont_ids) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean log p(cont_ids | enc_input_ids) directly on token IDs (no decode->encode).\n",
    "    Shapes:\n",
    "      enc_input_ids: [1, Lp]\n",
    "      cont_ids:      [1, Lc]\n",
    "    \"\"\"\n",
    "    input_ids = torch.cat([enc_input_ids, cont_ids], dim=1) #[1, Lp+Lc]\n",
    "    attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    compute_dtype = _model_compute_dtype(model)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=compute_dtype, enabled=compute_dtype != torch.float32):\n",
    "        logits = model(input_ids=input_ids[:, :-1], attention_mask=attn_mask[:, :-1]).logits  #[1, Lp+Lc-1, V]\n",
    "\n",
    "    label_start = enc_input_ids.shape[1]\n",
    "    seg_logits  = logits[:, label_start-1 : input_ids.shape[1]-1, :] #[1, Lc, V]\n",
    "    seg_target  = input_ids[:, label_start : ] #[1, Lc]\n",
    "    tok_logprobs = torch.log_softmax(seg_logits, dim=-1).gather(-1, seg_target.unsqueeze(-1)).squeeze(-1) #[1, Lc]\n",
    "    return float(tok_logprobs.sum().item()) / max(int(seg_target.numel()), 1)\n",
    "\n",
    "# Seeds & eval \n",
    "model.eval()\n",
    "torch.manual_seed(1234); random.seed(1234); np.random.seed(1234)\n",
    "try:\n",
    "    model.generation_config.trust_remote_code = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Run inference \n",
    "top_reasonings, final_answers = [], []\n",
    "beam_labels_col, beam_scores_col, all_beams_text = [], [], []\n",
    "\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    msgs = make_msgs_from_row(row)\n",
    "\n",
    "    gens, enc, cont_ids_list = gen_cot_beams(\n",
    "        msgs,\n",
    "        max_new_tokens=1100,\n",
    "        beams=11,\n",
    "        diversity_penalty=0.5,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=0,\n",
    "    )\n",
    "\n",
    "    # Compute external scores for each continuation using token IDs\n",
    "    enc_input_ids = enc[\"input_ids\"]\n",
    "    per_beam_scores = []\n",
    "    for cont_ids in cont_ids_list:\n",
    "        s = score_continuation_given_ids(model, enc_input_ids, cont_ids)\n",
    "        per_beam_scores.append(s)\n",
    "\n",
    "    # Parse labels from text (only for extracting <final>)\n",
    "    per_beam_labels = [parse_final_only(g) for g in gens]\n",
    "\n",
    "    # pick the most frequent non-empty label\n",
    "    counts = Counter([lab for lab in per_beam_labels if lab])\n",
    "    if counts:\n",
    "        top_lab, _ = counts.most_common(1)[0]\n",
    "        chosen_idx = next(i for i, lab in enumerate(per_beam_labels) if lab == top_lab)\n",
    "        chosen_label = top_lab\n",
    "    else:\n",
    "        # all labels empty → fall back to first beam\n",
    "        chosen_idx = 0\n",
    "        chosen_label = per_beam_labels[0] if per_beam_labels else \"\"\n",
    "\n",
    "    # Reasoning from the chosen beam\n",
    "    chosen_reasoning, _ = parse_analysis_final(gens[chosen_idx])\n",
    "\n",
    "    # Collect outputs\n",
    "    top_reasonings.append(chosen_reasoning)\n",
    "    final_answers.append(chosen_label)\n",
    "    beam_labels_col.append(json.dumps(per_beam_labels, ensure_ascii=False))\n",
    "    beam_scores_col.append(json.dumps(per_beam_scores, ensure_ascii=False))  # length-normalized logprobs\n",
    "    all_beams_text.append(gens)\n",
    "# Save\n",
    "OUT_CSV  = f\"{DATA_DIR}/val_preds_long_prompt_11beams.csv\"\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"reasoning\"]         = top_reasonings\n",
    "df_out[\"final_answer\"]      = final_answers # set by the consensus rule or max-confidence\n",
    "df_out[\"beam_final_labels\"] = beam_labels_col\n",
    "df_out[\"beam_sequence_scores\"] = beam_scores_col # mean log-probs\n",
    "max_beams = max(len(x) for x in all_beams_text)\n",
    "for j in range(max_beams):\n",
    "    df_out[f\"raw_generation{j+1}\"] = [row[j] if j < len(row) else \"\" for row in all_beams_text]\n",
    "\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth-oss20b)",
   "language": "python",
   "name": "unsloth-oss20b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
