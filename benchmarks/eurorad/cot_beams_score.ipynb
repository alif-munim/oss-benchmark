{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f8b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.5: Fast Gpt_Oss patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 4. Max memory: 47.383 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gpt_Oss does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f76b485156745189eec7a909e06087c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"                 \n",
    "os.environ[\"HF_PYTORCH_ATTENTION_BACKEND\"] = \"eager\" #force eager attention\n",
    "\n",
    "from torch.backends.cuda import sdp_kernel\n",
    "sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE = \"openai/gpt-oss-20b\"\n",
    "LORA_PATH = \"/home/alhusain/scratch/ondevice-llm/lora_cot_alhusain\"\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "# Load base model in 4-bit\n",
    "model_base, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=False,         \n",
    "    attn_implementation=\"eager\",   \n",
    "    float8_kv_cache=True,\n",
    ")\n",
    "\n",
    "# Attach LoRA adapter (inference)\n",
    "model = PeftModel.from_pretrained(model_base, LORA_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Tokenizer tweaks for generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "\n",
    "print(\"Ready. PEFT attached:\", isinstance(model, PeftModel))\n",
    "print(\"Active adapters:\", list(getattr(model, \"peft_config\", {}).keys()))\n",
    "print(\"Device:\", next(model.parameters()).device, \"| Max seq len:\", MAX_SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e3f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171 [00:00<?, ?it/s]Group Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n",
      "  5%|â–Œ         | 9/171 [07:03<2:10:09, 48.21s/it]"
     ]
    }
   ],
   "source": [
    "# Diverse-beam CoT inference with external per-beam scoring\n",
    "import re, json, math, torch, random, numpy as np, pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "_device = next(model.parameters()).device\n",
    "if _device.type == \"cuda\":\n",
    "    torch.cuda.set_device(_device.index or 0)\n",
    "\n",
    "DATA_DIR = \"/home/alhusain/scratch/ondevice-llm\"\n",
    "VAL_CSV  = f\"{DATA_DIR}/eurorad_val.csv\"\n",
    "\n",
    "AGREE_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "df = pd.read_csv(VAL_CSV)\n",
    "df = df.iloc[38:].copy()\n",
    "\n",
    "# Prompt \n",
    "def make_msgs_from_row(row):\n",
    "    post = str(row.get(\"PostDescription\", \"\")).strip()\n",
    "    ddx  = str(row.get(\"DifferentialDiagnosisList\", \"\")).strip()\n",
    "    user = (\n",
    "        \"You are an expert radiologist.\\n\"\n",
    "        \"Given a clinical case description and a finite list of candidate diagnoses,\\n\"\n",
    "        \"reason step by step using only the provided information, and then choose the single most likely final diagnosis FROM THE LIST.\\n\\n\"\n",
    "        f\"PostDescription: {post}\\n\"\n",
    "        f\"DifferentialDiagnosisList: {ddx}\\n\\n\"\n",
    "        \"Response rules:\\n\"\n",
    "        \"1) First, provide your reasoning inside <analysis> ... </analysis>. and keep it under 10 sentences.\\n\"\n",
    "        \"2) Then, provide the single chosen diagnosis inside <final> ... </final>.\\n\"\n",
    "        \"3) The <final> diagnosis must be copied VERBATIM from the DifferentialDiagnosisList.\\n\"\n",
    "        \"4) Do not include any text outside of the <analysis> and <final> blocks.\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert radiologist.\"},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "    ]\n",
    "\n",
    "# Parsers\n",
    "ANALYSIS_RE = re.compile(r\"<analysis>\\s*(.*?)\\s*</analysis>\", re.I | re.S)\n",
    "FINAL_RE    = re.compile(r\"<final>\\s*(.*?)\\s*</final>\",       re.I | re.S)\n",
    "\n",
    "def parse_analysis_final(text: str):\n",
    "    text = text or \"\"\n",
    "    a = (ANALYSIS_RE.search(text).group(1).strip()\n",
    "         if ANALYSIS_RE.search(text) else \"\")\n",
    "    f = (FINAL_RE.search(text).group(1).strip()\n",
    "         if FINAL_RE.search(text) else \"\")\n",
    "    if not f:\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        if lines: f = lines[-1]\n",
    "    return a, f\n",
    "\n",
    "def parse_final_only(text: str):\n",
    "    m = FINAL_RE.search(text or \"\")\n",
    "    if m: return m.group(1).strip()\n",
    "    lines = [ln.strip() for ln in (text or \"\").splitlines() if ln.strip()]\n",
    "    return lines[-1] if lines else \"\"\n",
    "\n",
    "# Utilities \n",
    "def _model_compute_dtype(model):\n",
    "    for p in model.parameters():\n",
    "        if p is not None:\n",
    "            return p.dtype\n",
    "    return torch.float32\n",
    "\n",
    "# Diverse beams (deterministic)     \n",
    "def gen_cot_beams(msgs, effort=\"high\", max_new_tokens=512,\n",
    "                  beams=8, diversity_penalty=0.35, length_penalty=1.0, no_repeat_ngram_size=0):\n",
    "    # Encode once; keep these tensors to later score beams under the same context\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=effort,\n",
    "    )\n",
    "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            do_sample=False,                   \n",
    "            num_beams=beams,\n",
    "            num_beam_groups=beams,             \n",
    "            num_return_sequences=beams,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # Return both texts (for parsing) and the raw token IDs (for scoring)\n",
    "    prompt_len = enc[\"input_ids\"].shape[1]\n",
    "    gens, cont_ids_list = [], []\n",
    "    for i in range(out.size(0)):\n",
    "        cont_ids = out[i, prompt_len:] # continuation ids\n",
    "        cont_ids_list.append(cont_ids.unsqueeze(0)) # [1, T_i]\n",
    "        gens.append(tokenizer.decode(cont_ids, skip_special_tokens=True).strip())\n",
    "    return gens, enc, cont_ids_list\n",
    "\n",
    "# External per-beam scoring (length-normalized logprob) \n",
    "@torch.no_grad()\n",
    "def score_continuation_given_ids(model, enc_input_ids, cont_ids) -> float:\n",
    "    \"\"\"\n",
    "    Compute mean log p(cont_ids | enc_input_ids) directly on token IDs (no decode->encode).\n",
    "    Shapes:\n",
    "      enc_input_ids: [1, Lp]\n",
    "      cont_ids:      [1, Lc]\n",
    "    \"\"\"\n",
    "    input_ids = torch.cat([enc_input_ids, cont_ids], dim=1) #[1, Lp+Lc]\n",
    "    attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    compute_dtype = _model_compute_dtype(model)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=compute_dtype, enabled=compute_dtype != torch.float32):\n",
    "        logits = model(input_ids=input_ids[:, :-1], attention_mask=attn_mask[:, :-1]).logits  #[1, Lp+Lc-1, V]\n",
    "\n",
    "    label_start = enc_input_ids.shape[1]\n",
    "    seg_logits  = logits[:, label_start-1 : input_ids.shape[1]-1, :] #[1, Lc, V]\n",
    "    seg_target  = input_ids[:, label_start : ] #[1, Lc]\n",
    "    tok_logprobs = torch.log_softmax(seg_logits, dim=-1).gather(-1, seg_target.unsqueeze(-1)).squeeze(-1) #[1, Lc]\n",
    "    return float(tok_logprobs.sum().item()) / max(int(seg_target.numel()), 1)\n",
    "\n",
    "# Seeds & eval \n",
    "model.eval()\n",
    "torch.manual_seed(1234); random.seed(1234); np.random.seed(1234)\n",
    "try:\n",
    "    model.generation_config.trust_remote_code = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Run inference \n",
    "top_reasonings, final_answers = [], []\n",
    "beam_labels_col, beam_scores_col, all_beams_text = [], [], []\n",
    "\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    msgs = make_msgs_from_row(row)\n",
    "\n",
    "    gens, enc, cont_ids_list = gen_cot_beams(\n",
    "        msgs,\n",
    "        max_new_tokens=512,\n",
    "        beams=18,\n",
    "        diversity_penalty=0.5,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=0,\n",
    "    )\n",
    "\n",
    "    # Compute external scores for each continuation using token IDs\n",
    "    enc_input_ids = enc[\"input_ids\"]\n",
    "    per_beam_scores = []\n",
    "    for cont_ids in cont_ids_list:\n",
    "        s = score_continuation_given_ids(model, enc_input_ids, cont_ids)\n",
    "        per_beam_scores.append(s)\n",
    "\n",
    "    # Parse labels once from text (only for extracting <final>)\n",
    "    per_beam_labels = [parse_final_only(g) for g in gens]\n",
    "\n",
    "    # consensus rule \n",
    "    counts = Counter([lab for lab in per_beam_labels if lab])\n",
    "    if counts:\n",
    "        top_lab, top_count = counts.most_common(1)[0]\n",
    "        ratio = top_count / len(per_beam_labels)\n",
    "    else:\n",
    "        top_lab, ratio = \"\", 0.0\n",
    "\n",
    "    if ratio >= AGREE_THRESHOLD and top_lab != \"\":\n",
    "        # Use consensus label; take reasoning from the earliest beam that predicted it\n",
    "        chosen_idx = next(i for i, lab in enumerate(per_beam_labels) if lab == top_lab)\n",
    "        chosen_label = top_lab\n",
    "    else:\n",
    "        # Use the most confident beam (highest mean log-prob)\n",
    "        chosen_idx = int(np.argmax(per_beam_scores))\n",
    "        chosen_label = per_beam_labels[chosen_idx]\n",
    "\n",
    "    # Reasoning from the chosen beam\n",
    "    chosen_reasoning, _ = parse_analysis_final(gens[chosen_idx])\n",
    "\n",
    "    # Collect outputs (lists -> JSON strings for CSV)\n",
    "    top_reasonings.append(chosen_reasoning)\n",
    "    final_answers.append(chosen_label)\n",
    "    beam_labels_col.append(json.dumps(per_beam_labels, ensure_ascii=False))\n",
    "    beam_scores_col.append(json.dumps(per_beam_scores, ensure_ascii=False))  # length-normalized logprobs\n",
    "    all_beams_text.append(\" ||| \".join(gens))\n",
    "\n",
    "# Save\n",
    "OUT_CSV  = f\"{DATA_DIR}/val_preds_cot_divbeam_extscores_long_prompt_0.5diversity.csv\"\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"reasoning\"]         = top_reasonings\n",
    "df_out[\"final_answer\"]      = final_answers # set by the consensus rule or max-confidence\n",
    "df_out[\"beam_final_labels\"] = beam_labels_col\n",
    "df_out[\"beam_sequence_scores\"] = beam_scores_col # our external scores (mean log-probs)\n",
    "df_out[\"raw_generations\"]   = all_beams_text\n",
    "\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV  = f\"{DATA_DIR}/val_preds_newscored_18beams_v2.csv\"\n",
    "\n",
    "df_out = df.head(17).copy()\n",
    "\n",
    "df_out[\"reasoning\"]         = top_reasonings\n",
    "df_out[\"final_answer\"]      = final_answers           \n",
    "df_out[\"beam_final_labels\"] = beam_labels_col\n",
    "df_out[\"beam_sequence_scores\"] = beam_scores_col\n",
    "df_out[\"raw_generations\"]   = all_beams_text\n",
    "\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth-oss20b)",
   "language": "python",
   "name": "unsloth-oss20b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
