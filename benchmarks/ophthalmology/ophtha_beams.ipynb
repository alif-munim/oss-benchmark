{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f06230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"HF_PYTORCH_ATTENTION_BACKEND\"] = \"eager\"  # force eager attention\n",
    "\n",
    "from torch.backends.cuda import sdp_kernel\n",
    "sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import re, json, math, torch, random, numpy as np, pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = \"openai/gpt-oss-20b\"\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "# ---------- Load base model in 4-bit ----------\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=False,\n",
    "    attn_implementation=\"eager\",\n",
    "    float8_kv_cache=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "\n",
    "print(\"Ready. Base model only (no LoRA).\")\n",
    "print(\"Device:\", next(model.parameters()).device, \"| Max seq len:\", MAX_SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63245fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### OPTHALMOLOGY BEAMS #####\n",
    "\n",
    "# Diverse-beam inference\n",
    "import re, json, math, torch, random, numpy as np, pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "_device = next(model.parameters()).device\n",
    "if _device.type == \"cuda\":\n",
    "    torch.cuda.set_device(_device.index or 0)\n",
    "\n",
    "DATA_DIR = \"/home/alhusain/scratch/ondevice-llm\"\n",
    "VAL_CSV = f\"{DATA_DIR}/ophthalmology_mcq.csv\"\n",
    "OUT_CSV = f\"{DATA_DIR}/final_results/oph_mcq_5beams.csv\"\n",
    "\n",
    "# Main hyperparams \n",
    "NUM_BEAMS = 5\n",
    "DIV_PENALTY = 0.5\n",
    "LENGTH_PENALTY = 1.0\n",
    "NO_REPEAT_NGRAM_SIZE = 0\n",
    "MAX_NEW_TOKENS = 3500  \n",
    "\n",
    "df = pd.read_csv(VAL_CSV)\n",
    "print(df.shape)\n",
    "df = df.iloc[0:].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Prompt (letters-only)\n",
    "# -------------------------\n",
    "SHORT_INSTR = (\n",
    "    \"You are an ophthalmology multiple-choice assistant.\\n\"\n",
    "    \"Output ONLY the correct answer letters (A–K), concatenated without spaces or punctuation.\\n\"\n",
    "    \"Examples: A, BD, ACE.\\n\"\n",
    "    \"Do not output any words, explanations, or reasoning.\\n\"\n",
    "    \"Your response MUST be letters only.\"\n",
    ")\n",
    "\n",
    "def make_msgs_from_row(row):\n",
    "    post = str(row.get(\"Question\", \"\")).strip()\n",
    "    user = f\"{SHORT_INSTR}\\n\\nQuestion: {post}\\n\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert ophthalmology assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": user},\n",
    "    ]\n",
    "\n",
    "# -------------------------\n",
    "# Parsers (letters only)\n",
    "# -------------------------\n",
    "ASSISTANTFINAL_TAG = re.compile(r\"assistant\\s*final\", re.I) \n",
    "\n",
    "def _normalize_choice_letters_only(text: str) -> str:\n",
    "    s = (text or \"\").upper()\n",
    "    # Allow A–K; dedupe & keep canonical A..K order\n",
    "    letters = re.findall(r\"[A-K]\", s)\n",
    "    if not letters:\n",
    "        return \"\"\n",
    "    ordered = [ch for ch in \"ABCDEFGHIJK\" if ch in letters]\n",
    "    seen, cleaned = set(), []\n",
    "    for ch in ordered:\n",
    "        if ch not in seen:\n",
    "            seen.add(ch)\n",
    "            cleaned.append(ch)\n",
    "    return \"\".join(cleaned)\n",
    "\n",
    "def parse_after_assistantfinal(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    m = ASSISTANTFINAL_TAG.search(text)\n",
    "    if not m:\n",
    "        # No assistantfinal marker -> failure\n",
    "        return \"\"\n",
    "    tail = text[m.end():]  # everything after 'assistantfinal'\n",
    "    tail = re.sub(r\"^[\\s:>\\]\\)\\-–—_]*\", \"\", tail)  # strip separators\n",
    "    m2 = re.search(r\"([A-Ka-k]{1,10})\", tail)\n",
    "    if not m2:\n",
    "        # Marker found but no A–K letters immediately after -> failure\n",
    "        return \"\"\n",
    "    return _normalize_choice_letters_only(m2.group(1))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Generation \n",
    "# -------------------------\n",
    "def gen_cot_beams(msgs, effort=\"medium\", max_new_tokens=8,\n",
    "                  beams=3, diversity_penalty=0.5, length_penalty=1.0, no_repeat_ngram_size=0):\n",
    "    # Encode once\n",
    "    enc = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        # reasoning_effort=effort, \n",
    "    )\n",
    "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            do_sample=False,\n",
    "            num_beams=beams,\n",
    "            num_beam_groups=beams,\n",
    "            num_return_sequences=beams,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    prompt_len = enc[\"input_ids\"].shape[1]\n",
    "    gens, cont_ids_list = [], []\n",
    "    for i in range(out.size(0)):\n",
    "        cont_ids = out[i, prompt_len:]\n",
    "        cont_ids_list.append(cont_ids.unsqueeze(0))\n",
    "        gens.append(tokenizer.decode(cont_ids, skip_special_tokens=True).strip())\n",
    "    return gens, enc, cont_ids_list\n",
    "\n",
    "# Seeds & eval\n",
    "model.eval()\n",
    "#torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "try:\n",
    "    model.generation_config.trust_remote_code = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Run inference\n",
    "final_answers = []\n",
    "beam_labels_col, all_beams_text = [], []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    msgs = make_msgs_from_row(row)\n",
    "\n",
    "    gens, _, _ = gen_cot_beams(\n",
    "        msgs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        beams=NUM_BEAMS,\n",
    "        diversity_penalty=DIV_PENALTY,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "    )\n",
    "\n",
    "    # Parse labels (letters only)\n",
    "    per_beam_labels = [parse_after_assistantfinal(g) for g in gens]\n",
    "\n",
    "    # -----------------------------\n",
    "    # PICK THE MOST FREQUENT LABEL\n",
    "    # Tie-breaker: earliest beam index among tied labels\n",
    "    # -----------------------------\n",
    "    counts = Counter([lab for lab in per_beam_labels if lab])\n",
    "    if counts:\n",
    "        max_count = max(counts.values())\n",
    "        tied_labels = [lab for lab, c in counts.items() if c == max_count]\n",
    "\n",
    "        if len(tied_labels) == 1:\n",
    "            chosen_label = tied_labels[0]\n",
    "        else:\n",
    "            # map each tied label -> earliest index where it appears\n",
    "            first_idx = {\n",
    "                lab: next(i for i, lab_i in enumerate(per_beam_labels) if lab_i == lab)\n",
    "                for lab in tied_labels\n",
    "            }\n",
    "            chosen_label = min(tied_labels, key=lambda lab: first_idx[lab])\n",
    "    else:\n",
    "        chosen_label = next((lab for lab in per_beam_labels if lab), \"\")\n",
    "\n",
    "    \n",
    "    final_answers.append(chosen_label)\n",
    "    beam_labels_col.append(json.dumps(per_beam_labels, ensure_ascii=False))\n",
    "    all_beams_text.append(\" ||| \".join(gens))\n",
    "\n",
    "# Save\n",
    "df_out = df.copy()\n",
    "df_out[\"final_answer\"] = final_answers\n",
    "df_out[\"beam_final_labels\"] = beam_labels_col\n",
    "df_out[\"raw_generations\"] = all_beams_text\n",
    "\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unsloth-oss20b)",
   "language": "python",
   "name": "unsloth-oss20b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
